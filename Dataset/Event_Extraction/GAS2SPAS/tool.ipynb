{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def merge_json_files(folder_path, output_file):\n",
    "    merged_data = []\n",
    "\n",
    "    # 遍历文件夹中的所有文件\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.json'):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            # 打开并读取JSON文件\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "                # 检查并提取字典中的内容\n",
    "                merged_data.extend(data)\n",
    "\n",
    "    # 将合并后的数据写入输出文件\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(merged_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "# 使用示例\n",
    "folder_path = '/home/qiang/projects/Digital_mental_health/Dataset/Event_Extraction/GAS2SPAS'  # 请替换为你的文件夹路径\n",
    "output_file = 'GAS_SPAS_800.json'  # 输出的合并文件名\n",
    "merge_json_files(folder_path, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Event: 100%|██████████| 2111/2111 [40:23<00:00,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理完成。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import Ollama\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "llm = Ollama(model=\"llama3:70b\")\n",
    "\n",
    "\n",
    "prompt_few_shot=\"An event is defined as any stimulus that an individual\\'s \\\n",
    "                environment or within that individual(e.g. thoughts of feelings) that has a good or bad effect from the individual\\'s \\\n",
    "                point of view. Events can be mental(e.g. I was afraid), social(e.g. I got a pay raise) or physical(e.g. I got in a car accident).\\\n",
    "                Events should be unambiguously good or bad from the individual\\'s point of view and may occur in the past, present or hypothetical future.  \\\n",
    "                Please analyze and determine the provided events. We should retain events that are unfavorable for the participants, i.e., bad events. \\\n",
    "                These events should not state facts and one's emotion or pose questions but rather describe a trigger for a bad situation for the participants. \\\n",
    "                Please keep such events and rewrite them into concise and fluent versions. For events that do not meet the requirements, please output:None\\\n",
    "                If there is no event in the post, you should output:\\\"Event:None\\\"\\\n",
    "                Please refer to the examples I gave:\\\n",
    "                <example>\\\n",
    "                Event: I snore REALLY loudly and sound like I'm not breathing, which means I must be completely unhealthy and will probably die young from some obscure disease, and no one will ever want to sleep next to me again.\\\n",
    "                Event: I snore really loudly and sound like I'm not breathing.\\\n",
    "                </example>\\\n",
    "                <example>\\\n",
    "                Event: I'm unable to work due to worsening disabilities and severe health issues, which means my whole life is basically over - no career success, no financial stability, no social status, and no hope for a better future.\\\n",
    "                Event: I'm unable to work due to worsening disabilities and severe health issues.\\\n",
    "                </example>\\\n",
    "                <example>\\\n",
    "                Event: why are you sad?\\\n",
    "                Event: None\\\n",
    "                </example>\\\n",
    "                <example>\\\n",
    "                Event: I bet you received lots of hit from that tweet; at work i cannot, wish i could\\\n",
    "                Event: I received hit from that tweet\\\n",
    "                </example>\\\n",
    "                <example>\\\n",
    "                Event: I`m so sad, really really sad\\\n",
    "                Event: None\\\n",
    "                </example>\\\n",
    "                You should answer with a specific format. For example, you should output:\\\"Event:...\\\"\\\n",
    "                Please understand what is event that we need from above explanation and extract the main event from the following paragraph:\"\n",
    "          \n",
    "\n",
    "def event_extraction(input_file, output_folder,posts):\n",
    "    data_list = []\n",
    "\n",
    "    for data in tqdm(posts, desc=\"Processing Event\"):\n",
    "        try:\n",
    "            prompt=prompt_few_shot+str(data)\n",
    "        except:\n",
    "            print(\"some errors\")\n",
    "        res = llm.predict(prompt)\n",
    "        result_str = res.split(\"Event:\", 1)[-1].strip()\n",
    "        data_res_dict = {\n",
    "            \"Post\": data,\n",
    "            \"Event\": result_str\n",
    "        }\n",
    "        data_list.append(data_res_dict)\n",
    "\n",
    "    output_json_path = os.path.join(output_folder, os.path.basename(input_file).split('.')[0]+'_event_extraction.json')\n",
    "\n",
    "\n",
    "    with open(output_json_path, 'w') as json_file:\n",
    "        json.dump(data_list, json_file, indent=4)\n",
    "\n",
    "\n",
    "def extract_text_in_json(json_file):\n",
    "    with open(json_file, 'r') as file:\n",
    "        try:\n",
    "            data = json.load(file)\n",
    "        except:\n",
    "            print(\"error\")\n",
    "        Post = [o[\"Post\"] for o in data]\n",
    "        # print(events)\n",
    "        return Post\n",
    "\n",
    "def extract_text_column(csv_file_path):\n",
    "    # 加载CSV文件\n",
    "    data = pd.read_csv(csv_file_path, encoding='latin1')\n",
    "    \n",
    "    # 检查\"text\"列是否存在于DataFrame中\n",
    "    if 'text' in data.columns:\n",
    "        # 提取\"text\"列\n",
    "        text_data = data['text']\n",
    "        return text_data\n",
    "    else:\n",
    "        raise ValueError(\"The CSV file does not contain a 'text' column.\")\n",
    "\n",
    "# 指定CSV文件的路径\n",
    "csv_file_path = '/home/qiang/projects/Digital_mental_health/Dataset/Event_Extraction/GAS2SPAS/GAS_SPAS_800.json'\n",
    "output_folder='/home/qiang/projects/Digital_mental_health/Dataset/Event_Extraction/GAS2SPAS/GAS2SPAS_event'\n",
    "# 调用函数并打印结果\n",
    "try:\n",
    "    text_column = extract_text_in_json(csv_file_path)\n",
    "    event_extraction(csv_file_path, output_folder, text_column)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "print(\"处理完成。\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "添加ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDs have been added successfully.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# 读取JSON文件\n",
    "with open('/home/qiang/projects/Digital_mental_health/Dataset/Event_Extraction/GAS2SPAS/GAS2SPAS_event/GAS_SPAS_800_event_extraction.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# 为每个元素添加唯一的ID\n",
    "for i, element in enumerate(data, start=1):\n",
    "    element['ID'] = f\"{i:05d}\"\n",
    "\n",
    "# 将更新后的数据写回JSON文件\n",
    "with open('/home/qiang/projects/Digital_mental_health/Dataset/Event_Extraction/GAS2SPAS/GAS2SPAS_event/GAS_SPAS_800_event_extraction.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"IDs have been added successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "2024-09-16 16:28:37.101915: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-16 16:28:37.137165: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-16 16:28:37.147454: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-16 16:28:37.213174: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-16 16:28:37.970694: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/opt/anaconda3/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering complete. Results saved to clustered_events.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from collections import defaultdict\n",
    "\n",
    "# 加载JSON文件\n",
    "with open('/home/qiang/projects/Digital_mental_health/Dataset/Event_Extraction/GAS2SPAS/GAS2SPAS_event/GAS_SPAS_800_event_extraction.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# 提取事件和ID\n",
    "events = [(item[\"ID\"], item[\"Event\"]) for item in data]\n",
    "\n",
    "# 初始化模型\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# 计算事件的句子嵌入\n",
    "embeddings = model.encode([event[1] for event in events], convert_to_tensor=True)\n",
    "\n",
    "# 计算相似度矩阵\n",
    "similarities = util.pytorch_cos_sim(embeddings, embeddings)\n",
    "\n",
    "# 聚类相似事件\n",
    "clusters = defaultdict(list)\n",
    "visited = set()\n",
    "\n",
    "# 设置相似度阈值\n",
    "threshold = 0.6\n",
    "\n",
    "for i in range(len(events)):\n",
    "    if i in visited:\n",
    "        continue\n",
    "    cluster = []\n",
    "    for j in range(len(events)):\n",
    "        if i != j and similarities[i][j] > threshold:\n",
    "            cluster.append({\n",
    "                \"ID\": events[j][0],\n",
    "                \"Event\": events[j][1],\n",
    "                \"Similarity_Score\": similarities[i][j].item()\n",
    "            })\n",
    "            visited.add(j)\n",
    "    if cluster:\n",
    "        # cluster.append({\n",
    "        #     \"ID\": events[i][0],\n",
    "        #     \"Event\": events[i][1],\n",
    "        #     \"Similarity_Score\": 1.0  # 自身与自身的相似度为1\n",
    "        # })\n",
    "        clusters[f\"Cluster_{i}\"] = cluster\n",
    "    visited.add(i)\n",
    "\n",
    "# 保存结果到新的JSON文件\n",
    "with open('/home/qiang/projects/Digital_mental_health/Dataset/Event_Extraction/GAS2SPAS/GAS2SPAS_event/GAS_SPAS_800_event_extraction_clustered.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(clusters, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"Clustering complete. Results saved to clustered_events.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理完成，结果已保存到 deduplication.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def extract_ids_from_json1(json1_data):\n",
    "    \"\"\"从第一个JSON文件中提取所有的ID\"\"\"\n",
    "    ids = set()\n",
    "    for cluster_key, events in json1_data.items():\n",
    "        for event in events:\n",
    "            ids.add(event.get(\"ID\"))\n",
    "    return ids\n",
    "\n",
    "def remove_elements_by_id(json2_data, ids_to_remove):\n",
    "    \"\"\"从第二个JSON文件中删除包含指定ID的元素\"\"\"\n",
    "    filtered_json2_data = [item for item in json2_data if item.get(\"ID\") not in ids_to_remove]\n",
    "    return filtered_json2_data\n",
    "\n",
    "# 读取第一个JSON文件\n",
    "with open('/home/qiang/projects/Digital_mental_health/Dataset/Event_Extraction/GAS2SPAS/GAS2SPAS_event/GAS_SPAS_800_event_extraction_clustered.json', 'r') as file1:\n",
    "    json1_data = json.load(file1)\n",
    "\n",
    "# 读取第二个JSON文件\n",
    "with open('/home/qiang/projects/Digital_mental_health/Dataset/Event_Extraction/GAS2SPAS/GAS2SPAS_event/GAS_SPAS_800_event_extraction.json', 'r') as file2:\n",
    "    json2_data = json.load(file2)\n",
    "\n",
    "# 提取第一个JSON文件中的所有ID\n",
    "ids_to_remove = extract_ids_from_json1(json1_data)\n",
    "\n",
    "# 删除第二个JSON文件中包含这些ID的元素\n",
    "updated_json2_data = remove_elements_by_id(json2_data, ids_to_remove)\n",
    "\n",
    "# 将结果写入新的JSON文件\n",
    "with open('/home/qiang/projects/Digital_mental_health/Dataset/Event_Extraction/GAS2SPAS/GAS2SPAS_event/GAS_SPAS_800_event_extraction_clustered_deduplication.json', 'w') as file:\n",
    "    json.dump(updated_json2_data, file, indent=4)\n",
    "\n",
    "print(\"处理完成，结果已保存到 deduplication.json\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
