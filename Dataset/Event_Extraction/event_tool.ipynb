{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON 文件中包含 714 个元素。\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# 读取 JSON 文件并解析为 Python 对象\n",
    "def count_elements_in_json(json_file):\n",
    "    with open(json_file, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "    return len(data)\n",
    "\n",
    "# 指定 JS/home/qiang/projects/Digital_mental_health/Dataset/Condidate_sentiment_datasetON 文件路径\n",
    "json_file = \"/home/qiang/projects/Digital_mental_health/Dataset/Event_Extraction/IAS2EAS/IAS2EAS_event/IAS_EAS_800_event_extraction_clustered_deduplication.json\"\n",
    "\n",
    "# 统计元素数量\n",
    "element_count = count_elements_in_json(json_file)\n",
    "print(f\"JSON 文件中包含 {element_count} 个元素。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prompt_few_shot=\"An event is defined as any stimulus that an individual\\'s \\\n",
    "                environment or within that individual(e.g. thoughts of feelings) that has a good or bad effect from the individual\\'s \\\n",
    "                point of view. Events can be mental(e.g. I was afraid), social(e.g. I got a pay raise) or physical(e.g. I got in a car accident).\\\n",
    "                Events should be unambiguously good or bad from the individual\\'s point of view and may occur in the past, present or hypothetical future.  \\\n",
    "                Please analyze and determine the provided events. We should retain events that are unfavorable for the participants, i.e., bad events. \\\n",
    "                These events should not state facts and one's emotion or pose questions but rather describe a trigger for a bad situation for the participants. \\\n",
    "                Please keep such events and rewrite them into concise and fluent versions. For events that do not meet the requirements, please output:None\\\n",
    "                If there is no event in the post, you should output:\\\"Event:None\\\"\\\n",
    "                Please refer to the examples I gave:\\\n",
    "                <example>\\\n",
    "                Event: I snore REALLY loudly and sound like I'm not breathing, which means I must be completely unhealthy and will probably die young from some obscure disease, and no one will ever want to sleep next to me again.\\\n",
    "                Event: I snore really loudly and sound like I'm not breathing.\\\n",
    "                </example>\\\n",
    "                <example>\\\n",
    "                Event: I'm unable to work due to worsening disabilities and severe health issues, which means my whole life is basically over - no career success, no financial stability, no social status, and no hope for a better future.\\\n",
    "                Event: I'm unable to work due to worsening disabilities and severe health issues.\\\n",
    "                </example>\\\n",
    "                <example>\\\n",
    "                Event: why are you sad?\\\n",
    "                Event: None\\\n",
    "                </example>\\\n",
    "                <example>\\\n",
    "                Event: I bet you received lots of hit from that tweet; at work i cannot, wish i could\\\n",
    "                Event: I received hit from that tweet\\\n",
    "                </example>\\\n",
    "                <example>\\\n",
    "                Event: I`m so sad, really really sad\\\n",
    "                Event: None\\\n",
    "                </example>\\\n",
    "                You should answer with a specific format. For example, you should output:\\\"Event:...\\\"\\\n",
    "                Please understand what is event that we need from above explanation and extract the main event from the following paragraph:\"\n",
    "          \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Event: 100%|██████████| 1852/1852 [43:00<00:00,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理完成。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import Ollama\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "llm = Ollama(model=\"llama3:70b\")\n",
    "\n",
    "\n",
    "prompt_few_shot=\"An event is defined as any stimulus that an individual\\'s \\\n",
    "                environment or within that individual(e.g. thoughts of feelings) that has a good or bad effect from the individual\\'s \\\n",
    "                point of view. Events can be mental(e.g. I was afraid), social(e.g. I got a pay raise) or physical(e.g. I got in a car accident).\\\n",
    "                Events should be unambiguously good or bad from the individual\\'s point of view and may occur in the past, present or hypothetical future.  \\\n",
    "                Please analyze and determine the provided events. We should retain events that are unfavorable for the participants, i.e., bad events. \\\n",
    "                These events should not state facts and one's emotion or pose questions but rather describe a trigger for a bad situation for the participants. \\\n",
    "                Please keep such events and rewrite them into concise and fluent versions. For events that do not meet the requirements, please output:None\\\n",
    "                If there is no event in the post, you should output:\\\"Event:None\\\"\\\n",
    "                Please refer to the examples I gave:\\\n",
    "                <example>\\\n",
    "                Event: I snore REALLY loudly and sound like I'm not breathing, which means I must be completely unhealthy and will probably die young from some obscure disease, and no one will ever want to sleep next to me again.\\\n",
    "                Event: I snore really loudly and sound like I'm not breathing.\\\n",
    "                </example>\\\n",
    "                <example>\\\n",
    "                Event: I'm unable to work due to worsening disabilities and severe health issues, which means my whole life is basically over - no career success, no financial stability, no social status, and no hope for a better future.\\\n",
    "                Event: I'm unable to work due to worsening disabilities and severe health issues.\\\n",
    "                </example>\\\n",
    "                <example>\\\n",
    "                Event: why are you sad?\\\n",
    "                Event: None\\\n",
    "                </example>\\\n",
    "                <example>\\\n",
    "                Event: I bet you received lots of hit from that tweet; at work i cannot, wish i could\\\n",
    "                Event: I received hit from that tweet\\\n",
    "                </example>\\\n",
    "                <example>\\\n",
    "                Event: I`m so sad, really really sad\\\n",
    "                Event: None\\\n",
    "                </example>\\\n",
    "                You should answer with a specific format. For example, you should output:\\\"Event:...\\\"\\\n",
    "                Please understand what is event that we need from above explanation and extract the main event from the following paragraph:\"\n",
    "          \n",
    "\n",
    "def event_extraction(input_file, output_folder,posts):\n",
    "    data_list = []\n",
    "\n",
    "    for data in tqdm(posts, desc=\"Processing Event\"):\n",
    "        try:\n",
    "            prompt=prompt_few_shot+str(data)\n",
    "        except:\n",
    "            print(\"some errors\")\n",
    "        res = llm.predict(prompt)\n",
    "        result_str = res.split(\"Event:\", 1)[-1].strip()\n",
    "        data_res_dict = {\n",
    "            \"Post\": data,\n",
    "            \"Event\": result_str\n",
    "        }\n",
    "        data_list.append(data_res_dict)\n",
    "\n",
    "    output_json_path = os.path.join(output_folder, os.path.basename(input_file).split('.')[0]+'_event_extraction.json')\n",
    "\n",
    "\n",
    "    with open(output_json_path, 'w') as json_file:\n",
    "        json.dump(data_list, json_file, indent=4)\n",
    "\n",
    "\n",
    "def extract_text_in_json(json_file):\n",
    "    with open(json_file, 'r') as file:\n",
    "        try:\n",
    "            data = json.load(file)\n",
    "        except:\n",
    "            print(\"error\")\n",
    "        Post = [o[\"Post\"] for o in data]\n",
    "        # print(events)\n",
    "        return Post\n",
    "\n",
    "def extract_text_column(csv_file_path):\n",
    "    # 加载CSV文件\n",
    "    data = pd.read_csv(csv_file_path, encoding='latin1')\n",
    "    \n",
    "    # 检查\"text\"列是否存在于DataFrame中\n",
    "    if 'text' in data.columns:\n",
    "        # 提取\"text\"列\n",
    "        text_data = data['text']\n",
    "        return text_data\n",
    "    else:\n",
    "        raise ValueError(\"The CSV file does not contain a 'text' column.\")\n",
    "\n",
    "# 指定CSV文件的路径\n",
    "csv_file_path = '/home/qiang/projects/Digital_mental_health/Dataset/Event_Extraction/GAS_SPAS_EVENT.json'\n",
    "output_folder='/home/qiang/projects/Digital_mental_health/Dataset/Event_Extraction/'\n",
    "# 调用函数并打印结果\n",
    "try:\n",
    "    text_column = extract_text_in_json(csv_file_path)\n",
    "    event_extraction(csv_file_path, output_folder, text_column)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "print(\"处理完成。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e944c238cef54b6882e4556f0ad9c34b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e406b263c86a4ead9bec70af971a4dff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04f8dd3a7c83440ab08959642029db3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f433965956848d6a289d461c9755a0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b02d9ddd24840c4b54564ec7b38d9d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19038911370d4c678258de65c8a268d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd5e19b8abdd4a7289597b1f481bc3dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bda6845723f49e296fb837690a723e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30e00849be2e4e99855ea2df0f9c8813",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "292eb0828a624225a14642762cd5d698",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3abab898e800443d8de5463c8561c09b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to sorted_events.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# 加载JSON文件\n",
    "with open('/home/qiang/projects/Digital_mental_health/Dataset/Event_Extraction/GAS_SPAS_EVENT_event_extraction.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# 提取所有事件\n",
    "events = [item[\"Event\"] for item in data]\n",
    "\n",
    "# 初始化模型\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# 计算事件的句子嵌入\n",
    "embeddings = model.encode(events, convert_to_tensor=True)\n",
    "\n",
    "# 计算相似度矩阵\n",
    "similarities = util.pytorch_cos_sim(embeddings, embeddings)\n",
    "\n",
    "# 对相似度进行排序\n",
    "sorted_indices = similarities.argsort(dim=-1, descending=True)\n",
    "\n",
    "# 构建结果数据\n",
    "sorted_events = []\n",
    "for i, event in enumerate(events):\n",
    "    similar_events = [{\"Similar_Event\": events[j], \"Score\": similarities[i][j].item()} \n",
    "                      for j in sorted_indices[i] if i != j]\n",
    "    sorted_events.append({\"Event\": event, \"Similar_Events\": similar_events})\n",
    "\n",
    "# 保存结果到新的JSON文件\n",
    "with open('sorted_events.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(sorted_events, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"Results saved to sorted_events.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing and splitting complete. Files saved.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# 加载JSON文件\n",
    "with open('sorted_events.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# 处理每个元素\n",
    "processed_data = []\n",
    "for item in data:\n",
    "    # 只保留前5个相似事件\n",
    "    top_5_similar_events = sorted(item[\"Similar_Events\"], key=lambda x: x[\"Score\"], reverse=True)[:5]\n",
    "    processed_item = {\n",
    "        \"Event\": item[\"Event\"],\n",
    "        \"Similar_Events\": top_5_similar_events\n",
    "    }\n",
    "    processed_data.append(processed_item)\n",
    "\n",
    "# 分割并保存每100个元素到一个JSON文件\n",
    "for i in range(0, len(processed_data), 100):\n",
    "    chunk = processed_data[i:i + 100]\n",
    "    file_name = f'processed_data_part_{i//100 + 1}.json'\n",
    "    with open(file_name, 'w', encoding='utf-8') as f:\n",
    "        json.dump(chunk, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"Processing and splitting complete. Files saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "写一个python程序，给当前json文件中每个element添加一个ID，从00001开始，下面是json中的一个element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDs have been added successfully.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# 读取JSON文件\n",
    "with open('/home/qiang/projects/Digital_mental_health/Dataset/Event_Extraction/GAS_SPAS_EVENT_event_extraction.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# 为每个元素添加唯一的ID\n",
    "for i, element in enumerate(data, start=1):\n",
    "    element['ID'] = f\"{i:05d}\"\n",
    "\n",
    "# 将更新后的数据写回JSON文件\n",
    "with open('/home/qiang/projects/Digital_mental_health/Dataset/Event_Extraction/GAS_SPAS_EVENT_event_extraction_.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"IDs have been added successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "1852it [00:14, 131.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering complete. Results saved to clustered_events.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# 加载JSON文件\n",
    "with open('/home/qiang/projects/Digital_mental_health/Dataset/Event_Extraction/GAS_SPAS_EVENT_event_extraction_.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# 提取所有事件和对应的ID\n",
    "events = [item[\"Event\"] for item in data]\n",
    "ids = [item[\"ID\"] for item in data]\n",
    "\n",
    "# 初始化模型\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# 计算事件的句子嵌入\n",
    "embeddings = model.encode(events, convert_to_tensor=True)\n",
    "\n",
    "# 计算相似度矩阵\n",
    "similarities = util.pytorch_cos_sim(embeddings, embeddings)\n",
    "\n",
    "# 设置相似度阈值\n",
    "threshold = 0.55\n",
    "\n",
    "# 聚类\n",
    "clusters = []\n",
    "visited = set()\n",
    "\n",
    "for i, event in tqdm(enumerate(events)):\n",
    "    if i in visited:\n",
    "        continue\n",
    "    # 创建新集群\n",
    "    cluster = {\"Cluster_Events\": [], \"IDs\": []}\n",
    "    for j in range(len(events)):\n",
    "        if similarities[i][j].item() > threshold:\n",
    "            cluster[\"Cluster_Events\"].append(events[j])\n",
    "            cluster[\"IDs\"].append(ids[j])\n",
    "            visited.add(j)\n",
    "    clusters.append(cluster)\n",
    "\n",
    "# 保存结果到新的JSON文件\n",
    "with open('clustered_events.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(clusters, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"Clustering complete. Results saved to clustered_events.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering complete. Results saved to clustered_events.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from collections import defaultdict\n",
    "\n",
    "# 加载JSON文件\n",
    "with open('GAS_SPAS_EVENT_event_extraction_.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# 提取事件和ID\n",
    "events = [(item[\"ID\"], item[\"Event\"]) for item in data]\n",
    "\n",
    "# 初始化模型\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# 计算事件的句子嵌入\n",
    "embeddings = model.encode([event[1] for event in events], convert_to_tensor=True)\n",
    "\n",
    "# 计算相似度矩阵\n",
    "similarities = util.pytorch_cos_sim(embeddings, embeddings)\n",
    "\n",
    "# 聚类相似事件\n",
    "clusters = defaultdict(list)\n",
    "visited = set()\n",
    "\n",
    "# 设置相似度阈值\n",
    "threshold = 0.6\n",
    "\n",
    "for i in range(len(events)):\n",
    "    if i in visited:\n",
    "        continue\n",
    "    cluster = []\n",
    "    for j in range(len(events)):\n",
    "        if i != j and similarities[i][j] > threshold:\n",
    "            cluster.append({\n",
    "                \"ID\": events[j][0],\n",
    "                \"Event\": events[j][1],\n",
    "                \"Similarity_Score\": similarities[i][j].item()\n",
    "            })\n",
    "            visited.add(j)\n",
    "    if cluster:\n",
    "        # cluster.append({\n",
    "        #     \"ID\": events[i][0],\n",
    "        #     \"Event\": events[i][1],\n",
    "        #     \"Similarity_Score\": 1.0  # 自身与自身的相似度为1\n",
    "        # })\n",
    "        clusters[f\"Cluster_{i}\"] = cluster\n",
    "    visited.add(i)\n",
    "\n",
    "# 保存结果到新的JSON文件\n",
    "with open('clustered_events_.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(clusters, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"Clustering complete. Results saved to clustered_events.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "写一个python程序，只保留similarity 0.6以上的，json文件中的其中一个element如下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering complete. Results saved to filtered_clustered_events.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# 加载JSON文件\n",
    "with open('/home/qiang/projects/Digital_mental_health/Dataset/Event_Extraction/clustered_events_0_55.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# 过滤相似度小于0.6的元素\n",
    "filtered_data = {}\n",
    "for cluster_id, events in data.items():\n",
    "    filtered_events = [event for event in events if event[\"Similarity_Score\"] >= 0.6]\n",
    "    if filtered_events:  # 如果过滤后仍有事件，才保留该群组\n",
    "        filtered_data[cluster_id] = filtered_events\n",
    "\n",
    "# 保存结果到新的JSON文件\n",
    "with open('clustered_events_0_6.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(filtered_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"Filtering complete. Results saved to filtered_clustered_events.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面是一个Python程序，它可以遍历JSON文件，并将每个element中最后一个 Similarity_Score 为 1.0 的项目删除："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理完成，结果已保存到output.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def remove_last_similarity_score(json_data):\n",
    "    # 遍历所有的cluster\n",
    "    for cluster_key, events in json_data.items():\n",
    "        # 找到最后一个 similarity_score == 1.0 的索引\n",
    "        index_to_remove = None\n",
    "        for i, event in enumerate(events):\n",
    "            if event.get(\"Similarity_Score\") == 1.0:\n",
    "                index_to_remove = i\n",
    "\n",
    "        # 如果找到了这样的event，删除它\n",
    "        if index_to_remove is not None:\n",
    "            del json_data[cluster_key][index_to_remove]\n",
    "\n",
    "    return json_data\n",
    "\n",
    "# 读取json文件\n",
    "with open('/home/qiang/projects/Digital_mental_health/Dataset/Event_Extraction/clustered_events_0_6.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# 调用函数处理数据\n",
    "updated_data = remove_last_similarity_score(data)\n",
    "\n",
    "# 将结果写入新的json文件\n",
    "with open('clustered_events_0_6_final.json', 'w') as file:\n",
    "    json.dump(updated_data, file, indent=4)\n",
    "\n",
    "print(\"处理完成，结果已保存到output.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据重复的ID，进行去重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理完成，结果已保存到 deduplication.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def extract_ids_from_json1(json1_data):\n",
    "    \"\"\"从第一个JSON文件中提取所有的ID\"\"\"\n",
    "    ids = set()\n",
    "    for cluster_key, events in json1_data.items():\n",
    "        for event in events:\n",
    "            ids.add(event.get(\"ID\"))\n",
    "    return ids\n",
    "\n",
    "def remove_elements_by_id(json2_data, ids_to_remove):\n",
    "    \"\"\"从第二个JSON文件中删除包含指定ID的元素\"\"\"\n",
    "    filtered_json2_data = [item for item in json2_data if item.get(\"ID\") not in ids_to_remove]\n",
    "    return filtered_json2_data\n",
    "\n",
    "# 读取第一个JSON文件\n",
    "with open('clustered_events_0_6_final.json', 'r') as file1:\n",
    "    json1_data = json.load(file1)\n",
    "\n",
    "# 读取第二个JSON文件\n",
    "with open('GAS_SPAS_EVENT_event_extraction_.json', 'r') as file2:\n",
    "    json2_data = json.load(file2)\n",
    "\n",
    "# 提取第一个JSON文件中的所有ID\n",
    "ids_to_remove = extract_ids_from_json1(json1_data)\n",
    "\n",
    "# 删除第二个JSON文件中包含这些ID的元素\n",
    "updated_json2_data = remove_elements_by_id(json2_data, ids_to_remove)\n",
    "\n",
    "# 将结果写入新的JSON文件\n",
    "with open('GAS_SPAS_EVENT_event_extraction_deduplication.json', 'w') as file:\n",
    "    json.dump(updated_json2_data, file, indent=4)\n",
    "\n",
    "print(\"处理完成，结果已保存到 deduplication.json\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
